

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Clustering &mdash; ubic_crash_course 1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Phylogenetics" href="10_Phylogenetics.html" />
    <link rel="prev" title="Alignment (Part I)" href="8.1_Alignment.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> ubic_crash_course
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Lessons</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="1_Welcome.html">Hello World: What’s the purpose of bioinformatics?</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_LinuxTerminal.html">The Linux Terminal</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_AdvancedTerminal.html">Advanced Command Line + Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_Python.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_Biopython.html">Bioinformatics x Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_Challenge1.html">Challenge 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="8.1_Alignment.html">Alignment (Part I)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Clustering</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#skills-familiarity-with-sklearn-package-and-an-understanding-of-why-clustering-is-useful-in-bioinformatics">Skills: Familiarity with sklearn package and an understanding of why clustering is useful in bioinformatics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#syntax-note">Syntax note:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#general-factoids">General Factoids</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bioinformatics-applications-of-clustering">Bioinformatics Applications of Clustering:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#generate-data">Generate Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cluster-the-data">Cluster the Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-means">1. K-means</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#steps">Steps:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#code">Code:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#agglomerative-hierarchical">2. Agglomerative Hierarchical</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#description">Description:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Code:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#soft-clustering">3. Soft Clustering</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id2">Description:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">Code:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#challenge">Challenge</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="10_Phylogenetics.html">Phylogenetics</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_Challenge2.html">Challenge 2</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ubic_crash_course</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Clustering</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/9_1_Clustering.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="clustering">
<h1>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h1>
<div class="section" id="skills-familiarity-with-sklearn-package-and-an-understanding-of-why-clustering-is-useful-in-bioinformatics">
<h2>Skills: Familiarity with sklearn package and an understanding of why clustering is useful in bioinformatics<a class="headerlink" href="#skills-familiarity-with-sklearn-package-and-an-understanding-of-why-clustering-is-useful-in-bioinformatics" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="syntax-note">
<h2>Syntax note:<a class="headerlink" href="#syntax-note" title="Permalink to this headline">¶</a></h2>
<p>Note that in Python syntax, you will have to use a dot to specify where you want to get a function from. For example, there is a package called numpy, with a module called random, with a function called seed. The seed function is called by typing out <code class="docutils literal notranslate"><span class="pre">np.random.seed()</span></code>, telling python where exactly it needs to look.</p>
<p><strong>WE ARE USING PYTHON3, NOT PYTHON</strong></p>
</div>
<div class="section" id="general-factoids">
<h2>General Factoids<a class="headerlink" href="#general-factoids" title="Permalink to this headline">¶</a></h2>
<p><strong>Clustering:</strong> The process of partitioning a dataset into groups based on shared unkown characteristics. In other words, we are looking for patterns in data where we do not necessarily know the patterns to look for ahead of time.</p>
<p><strong>In:</strong> N points, usually in the form of a matrix(each row is a point).</p>
<p><strong>In:</strong> A distance function to tell us how similar two points are. The most intuitive distance function is <a class="reference external" href="http://rosalind.info/glossary/euclidean-distance/">euclidean distance</a>. The type of distance measure you use can vary depending on the type of data you are using. One specific example of how fine tuned distance metrics can be is a distance metric that weights substitutions in DNA strings heavier than indels because indels are more common sequencing errors. This means that such a metric would not separate two DNA strands due to sequencing error.</p>
<p><strong>Out:</strong> K groups, each containing points which are similar to each other in some way. Some algorithms cluster for a preset number K, others figure it out as they go along.</p>
<p><strong>How is DNA a point?</strong> One common way we translate DNA strings to points is by making a string of DNA into a kmer vector. A kmer is a string of length k, and there are 4^k possible kmers in any DNA strand. Usually we think of kmers as ordered lexicographically like this: AAA, AAC, AAG, AAT, ACA, ACC, ACG, ACT, AGA and so on. One way to represent a DNA strand is to create an array of zeros of length 4^k and increment by one for each time that kmer appears in the DNA strand.</p>
<p><em>Problem:</em> Find the kmer vector of “AG” (k=2).</p>
</div>
<div class="section" id="bioinformatics-applications-of-clustering">
<h2>Bioinformatics Applications of Clustering:<a class="headerlink" href="#bioinformatics-applications-of-clustering" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Finding what genes are up and down regulated under certain conditions. Imagine you have a matrix, where each point is a set of gene expression recorded for a variety of conditions. If two points are close to each other, that means they had similar expression levels throughout those conditions.</p></li>
<li><p>Discerning different species present in a sample of unkown contents. This can be done with an algorithm that does not have a preditermined amount of clusters. An example application is taking HIV sequences from a patient, clustering them, and filtering clusters under a certain size to find the sequences of prevalent strains within the patient.</p></li>
<li><p>Finding evolutionary relationships between samples using hierarchical clustering. The earlier on two centers were combined, the closer their corresponding points are from an evolutionary perspective.</p></li>
</ol>
</div>
<div class="section" id="generate-data">
<h2>Generate Data<a class="headerlink" href="#generate-data" title="Permalink to this headline">¶</a></h2>
<p>In order to demonstrate the differences between different clustering methods, we need datasets that cluster differently depending on technique.</p>
<p>Copy the starter file into your directory:</p>
<p><code class="docutils literal notranslate"><span class="pre">/home/ubuntu/clustering_lesson/clust.py</span></code></p>
<p>This file contains a plotting function - don’t touch it! We will be creating stuff for that plotting function to plot in a minute, so write your code below that function.</p>
<p>Start by creating some blobs using numpy’s random normal generator. First, create a seed for the random number generator with</p>
<p><code class="docutils literal notranslate"><span class="pre">np.random.seed()</span></code></p>
<p>Make a few blobs of points (probably two) with centers between 0 and 20 and varied stdevs (keeping the array dimensions the same). Example syntax:
<code class="docutils literal notranslate"><span class="pre">clust1</span> <span class="pre">=</span> <span class="pre">np.random.normal(5,</span> <span class="pre">2,</span> <span class="pre">(1000,2))</span></code></p>
<p>Put the point blobs into one structure so that we can cluster them all at once with</p>
<p><code class="docutils literal notranslate"><span class="pre">dataset1</span> <span class="pre">=</span> <span class="pre">np.concatenate()</span></code></p>
<p>We will be comparing how our point blobs cluster to the way that circles of datapoints cluster. Create two concentric circles as the second dataset.
<code class="docutils literal notranslate"><span class="pre">dataset2</span> <span class="pre">=</span> <span class="pre">datasets.make_circles(n_samples=1000,</span> <span class="pre">factor=.5,</span> <span class="pre">noise=.05)[0]</span></code></p>
<p>If you want to see what your datasets look like, use the <code class="docutils literal notranslate"><span class="pre">cluster_plots</span></code> function</p>
<p><strong>Note:</strong> <code class="docutils literal notranslate"><span class="pre">cluster_plots</span></code> will save your plots to a pdf file in your current directory, the name of which can be changed. You’ll have to scp the file over to your computer to view it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scp</span> <span class="n">username</span><span class="nd">@ec2</span><span class="o">-</span><span class="mi">3</span><span class="o">-</span><span class="mi">135</span><span class="o">-</span><span class="mi">188</span><span class="o">-</span><span class="mf">28.</span><span class="n">us</span><span class="o">-</span><span class="n">east</span><span class="o">-</span><span class="mf">2.</span><span class="n">compute</span><span class="o">.</span><span class="n">amazonaws</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">username</span><span class="o">/</span><span class="n">path</span> <span class="o">/</span><span class="n">localpath</span><span class="o">/</span>
</pre></div>
</div>
</div>
<div class="section" id="cluster-the-data">
<h2>Cluster the Data<a class="headerlink" href="#cluster-the-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="k-means">
<h3>1. K-means<a class="headerlink" href="#k-means" title="Permalink to this headline">¶</a></h3>
<div class="section" id="steps">
<h4>Steps:<a class="headerlink" href="#steps" title="Permalink to this headline">¶</a></h4>
<p>   I. Select K centers. This can be done a variety of ways, the easiest of which is to select a random point, find the farthest point from that and select it, find the furthest point from the previous and so on.</p>
<p>   II. Assign each point to the center nearest to it.</p>
<p>   III. For each center, take all the points attached to it and take their average to create a new center for that cluster.</p>
<p>   IV. Reassign all points to their nearest center and continue reassigning + averaging until the iteration when nothing changes</p>
</div>
<div class="section" id="code">
<h4>Code:<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h4>
<p>We will be using the KMeans algorithm from sklearn’s cluster package to dataset1 and dataset2. Remember that k is the number of clusters the KMeans algorithm will assign points to, so give Kmeans the appropriate k. The points returned from the KMeans function will be separated into clusters, making them easy for the cluster_plots function to distinguish and color accordingly.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kmeans_dataset1</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">dataset1</span><span class="p">)</span>
<span class="n">cluster_plots</span><span class="p">(</span><span class="n">dataset1</span><span class="p">,</span> <span class="n">dataset2</span><span class="p">,</span> <span class="n">kmeans_dataset1</span><span class="p">,</span> <span class="n">kmeans_dataset2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="agglomerative-hierarchical">
<h3>2. Agglomerative Hierarchical<a class="headerlink" href="#agglomerative-hierarchical" title="Permalink to this headline">¶</a></h3>
<div class="section" id="description">
<h4>Description:<a class="headerlink" href="#description" title="Permalink to this headline">¶</a></h4>
<p>   I. Start with every point being its very own cluster center.</p>
<p>   II. Find the two centers which are closest to each other and combine them into one cluster. The new center is the average of the two previous ones.</p>
<p>   III. Continue combining the closest centers until all points are under a single cluster.</p>
</div>
<div class="section" id="id1">
<h4>Code:<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Let’s look at the blobs and circles we made again. Look at the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html">documentation page for agglomerative clustering</a> and figure out the proper calls for the two datasets (syntax is very similar to what you did for k-means).</p>
<p>Now, run <code class="docutils literal notranslate"><span class="pre">cluster_plots()</span></code> to graph and <code class="docutils literal notranslate"><span class="pre">scp</span></code> to view the results. Do you notice a difference in quality of the clustering of the left and right graphs?</p>
<p>Unfortunately, this still does not solve our circlular cluster issue. For that, we can ask the sklearn package to build a graph out of our circles which restricts the amount of nearest neighbors a point can cluster with.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#this line limits the number of nearest neighbors to 6</span>
<span class="n">connect</span> <span class="o">=</span> <span class="n">kneighbors_graph</span><span class="p">(</span><span class="n">dataset2</span><span class="p">,</span> <span class="n">include_self</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span>

<span class="c1">#in this line, you need to set linkage to complete, number of clusters to 2, and set connectivity equal to the graph </span>
<span class="c1">#on the previous line</span>
<span class="n">hc_dataset2_connectivity</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">AgglomerativeClustering</span><span class="p">()</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">dataset2</span><span class="p">)</span>
</pre></div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">cluster_plots()</span></code> to graph the plot resulting from the regular AgglomerativeClustering beside the graph that plots the connected AgglomerativeClustering. As you may have guessed, this fixes the circle problem.</p>
</div>
</div>
<div class="section" id="soft-clustering">
<h3>3. Soft Clustering<a class="headerlink" href="#soft-clustering" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id2">
<h4>Description:<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>This is essentially the same thing as k-means, but points cannot be hard assigned to a cluster. Instead, each point has a probability of being in each cluster, and points with higher probabilities influence the center of that cluster more.</p>
<p>I. Select K centers randomly (or slightly less randomly, like <a class="reference external" href="https://en.wikipedia.org/wiki/Farthest-first_traversal">farthest first traversal</a>.</p>
<p>II. Expectation step: Assign a responsibility (a likelyhood) for each point in respect to each cluster. Basically, the closer a point is to a center the higher the likelyhood of that point. This can be calculated in a variety of ways, but the main idea is that it is some function relating the distance from a point to a center to the distances between all points and that center to see if it is much closer or further than other points</p>
<p>III. Maximization step: compute new cluster centers by using a <strong>weighted</strong> average of the points based on the likelyhoods calculated in step II.</p>
</div>
<div class="section" id="id3">
<h4>Code:<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>Let’s go ahead and see what will happen with our blobs and circles under this clustering algorithm:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">em_set1</span><span class="o">=</span><span class="n">mixture</span><span class="o">.</span><span class="n">GaussianMixture</span><span class="p">()</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">()</span>
</pre></div>
</div>
<p>Use these new clusters in our <code class="docutils literal notranslate"><span class="pre">cluster_plots()</span></code></p>
</div>
</div>
</div>
<div class="section" id="challenge">
<h2>Challenge<a class="headerlink" href="#challenge" title="Permalink to this headline">¶</a></h2>
<p>Head <a class="reference external" href="https://github.com/sabeelmansuri/Bioinformatics-Crash-Course/blob/master/9_2_Dirichlet_Challenge">over here</a> to complete our clustering challenge!</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="10_Phylogenetics.html" class="btn btn-neutral float-right" title="Phylogenetics" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="8.1_Alignment.html" class="btn btn-neutral float-left" title="Alignment (Part I)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>